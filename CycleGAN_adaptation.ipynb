{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e1bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:15.767075Z",
     "iopub.status.busy": "2025-02-23T15:33:15.766846Z",
     "iopub.status.idle": "2025-02-23T15:33:22.892162Z",
     "shell.execute_reply": "2025-02-23T15:33:22.891467Z"
    },
    "papermill": {
     "duration": 7.130402,
     "end_time": "2025-02-23T15:33:22.893738",
     "exception": false,
     "start_time": "2025-02-23T15:33:15.763336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,root,transform=None,mode='train'):\n",
    "        self.transform= transform\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root,'%sA' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root,'%sB' % mode) + '/*.*'))\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        item_A= Image.open(self.files_A[index % len(self.files_A)])\n",
    "\n",
    "        item_B=Image.open(self.files_B[random.randint(0,len(self.files_B)-1)])\n",
    "\n",
    "        if self.transform:\n",
    "            item_A=self.transform(item_A)\n",
    "            item_B=self.transform(item_B)\n",
    "\n",
    "        return item_A, item_B\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A),len(self.files_B))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24fe54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:22.900584Z",
     "iopub.status.busy": "2025-02-23T15:33:22.900259Z",
     "iopub.status.idle": "2025-02-23T15:33:22.905114Z",
     "shell.execute_reply": "2025-02-23T15:33:22.904278Z"
    },
    "papermill": {
     "duration": 0.009568,
     "end_time": "2025-02-23T15:33:22.906445",
     "exception": false,
     "start_time": "2025-02-23T15:33:22.896877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "\n",
    "        conv_block = [ nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features,in_features,3),\n",
    "                                nn.InstanceNorm2d(in_features),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.ReflectionPad2d(1),\n",
    "                                nn.Conv2d(in_features,in_features,3),\n",
    "                                nn.InstanceNorm2d(in_features)]\n",
    "        self.conv_block=nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        return x+self.conv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82ed68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:22.912673Z",
     "iopub.status.busy": "2025-02-23T15:33:22.912424Z",
     "iopub.status.idle": "2025-02-23T15:33:22.918618Z",
     "shell.execute_reply": "2025-02-23T15:33:22.917838Z"
    },
    "papermill": {
     "duration": 0.01073,
     "end_time": "2025-02-23T15:33:22.919934",
     "exception": false,
     "start_time": "2025-02-23T15:33:22.909204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,input_nc,output_nc,n_residual_blocks=4):\n",
    "        super(Generator,self).__init__()\n",
    "\n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc,64,7),\n",
    "                    nn.InstanceNorm2d(64),\n",
    "                    nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        in_features=64\n",
    "        out_features=in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [ nn.Conv2d(in_features,out_features,3,stride=2,padding=1),\n",
    "                      nn.InstanceNorm2d(out_features),\n",
    "                      nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features=out_features\n",
    "            out_features=in_features*2\n",
    "        \n",
    "        for _ in range (n_residual_blocks):\n",
    "            model+= [ResidualBlock(in_features)]\n",
    "        \n",
    "        out_features=in_features//2\n",
    "        for _ in range(2):\n",
    "            model+= [nn.ConvTranspose2d(in_features,out_features,3,stride=2,padding=1,output_padding=1),\n",
    "                     nn.InstanceNorm2d(out_features),\n",
    "                     nn.ReLU(inplace=True)\n",
    "                     ] ##BOOOH\n",
    "            in_features=out_features\n",
    "            out_features=in_features//2\n",
    "        \n",
    "        model+= [ nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(64,output_nc,7),\n",
    "                 nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model=nn.Sequential(*model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a8a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:22.926166Z",
     "iopub.status.busy": "2025-02-23T15:33:22.925968Z",
     "iopub.status.idle": "2025-02-23T15:33:22.931175Z",
     "shell.execute_reply": "2025-02-23T15:33:22.930629Z"
    },
    "papermill": {
     "duration": 0.009707,
     "end_time": "2025-02-23T15:33:22.932316",
     "exception": false,
     "start_time": "2025-02-23T15:33:22.922609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,input_nc):\n",
    "        super(Discriminator,self).__init__()\n",
    "\n",
    "        model = [ nn.Conv2d(input_nc,64,4,stride=2,padding=1),\n",
    "                 nn.LeakyReLU(0.2,inplace=True)]\n",
    "        \n",
    "        model += [ nn.Conv2d(64,128,4,stride=2,padding=1),\n",
    "                  nn.InstanceNorm2d(128),\n",
    "                  nn.LeakyReLU(0.2,inplace=True)]\n",
    "        \n",
    "        model += [ nn.Conv2d(128,256,4,stride=2,padding=1),\n",
    "                  nn.InstanceNorm2d(256),\n",
    "                  nn.LeakyReLU(0.2,inplace=True)]\n",
    "        \n",
    "        model += [ nn.Conv2d(256,512,4,stride=2,padding=1),\n",
    "                  nn.InstanceNorm2d(512),\n",
    "                  nn.LeakyReLU(0.2,inplace=True)]\n",
    "        \n",
    "        model += [ nn.Conv2d(512,1,4,padding=1)]\n",
    "\n",
    "        self.model= nn.Sequential(*model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x= self.model(x)\n",
    "        return F.avg_pool2d(x,x.size()[2:]).view(x.size()[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8c6d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:22.938280Z",
     "iopub.status.busy": "2025-02-23T15:33:22.938086Z",
     "iopub.status.idle": "2025-02-23T15:33:22.941690Z",
     "shell.execute_reply": "2025-02-23T15:33:22.941095Z"
    },
    "papermill": {
     "duration": 0.007966,
     "end_time": "2025-02-23T15:33:22.942946",
     "exception": false,
     "start_time": "2025-02-23T15:33:22.934980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LambdaLR():\n",
    "    def __init__(self,n_epochs,offset,decay_start_epoch):\n",
    "        assert ((n_epochs-decay_start_epoch)>0), \"Decay must start before training session ends\"\n",
    "        self.n_epochs=n_epochs\n",
    "        self.offset=offset\n",
    "        self.decay_start_epoch=decay_start_epoch\n",
    "    \n",
    "    def step(self,epoch):\n",
    "        return 1.0-max(0,epoch+self.offset-self.decay_start_epoch)/(self.n_epochs-self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d6cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:22.949093Z",
     "iopub.status.busy": "2025-02-23T15:33:22.948899Z",
     "iopub.status.idle": "2025-02-23T15:33:22.952604Z",
     "shell.execute_reply": "2025-02-23T15:33:22.951842Z"
    },
    "papermill": {
     "duration": 0.00817,
     "end_time": "2025-02-23T15:33:22.953869",
     "exception": false,
     "start_time": "2025-02-23T15:33:22.945699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname= m.__class__.__name__\n",
    "    if classname.find('Conv')!=-1:\n",
    "        m.weight.data.normal_(0,0.02)\n",
    "    if classname.find('BatchNorm2d')!=-1:\n",
    "        m.weight.data.normal_(1.0,0.02)\n",
    "        m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d6b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:22.960001Z",
     "iopub.status.busy": "2025-02-23T15:33:22.959807Z",
     "iopub.status.idle": "2025-02-23T15:33:27.031937Z",
     "shell.execute_reply": "2025-02-23T15:33:27.030867Z"
    },
    "papermill": {
     "duration": 4.076985,
     "end_time": "2025-02-23T15:33:27.033520",
     "exception": false,
     "start_time": "2025-02-23T15:33:22.956535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    " \n",
    "class CycleGAN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_nc=3,  # numero di canali in input\n",
    "                 output_nc=3,  # numero di canali di output\n",
    "                 image_size=256,  # le dimensioni dell'immagine\n",
    "                 lr=0.0002,  # learning rate\n",
    "                 beta1=0.5, beta2=0.999,  # valori beta dell'Adam\n",
    "                 starting_epoch=0,  # epoca di partenza se si inizia facendo resume di un training interrotto\n",
    "                 n_epochs=200,  # numero totale di epoche\n",
    "                 decay_epoch=100,  # epoca dopo la quale iniziare a far scendere il learning rate\n",
    "                 data_root='your_dataset',  # cartella in cui si trovano i dati\n",
    "                 batch_size=1,  # batch size\n",
    "                 num_workers=4):  # numero di thread per il dataloader\n",
    " \n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    " \n",
    "        # Definizione dei generatori: da A a B e da B ad A\n",
    "        self.netG_A2B = Generator(input_nc, output_nc)\n",
    "        self.netG_B2A = Generator(output_nc, input_nc)\n",
    " \n",
    "        # Definizione dei discriminatori: uno per A e uno per B\n",
    "        self.netD_A = Discriminator(input_nc)\n",
    "        self.netD_B = Discriminator(output_nc)\n",
    " \n",
    "        # Applicazione della normalizzazione\n",
    "        self.netG_A2B.apply(weights_init_normal)\n",
    "        self.netG_B2A.apply(weights_init_normal)\n",
    "        self.netD_A.apply(weights_init_normal)\n",
    "        self.netD_B.apply(weights_init_normal)\n",
    " \n",
    "        # Definizione delle loss\n",
    "        self.criterion_GAN = torch.nn.MSELoss()\n",
    "        self.criterion_cycle = torch.nn.L1Loss()\n",
    "        self.criterion_identity = torch.nn.L1Loss()\n",
    " \n",
    "        # Disabilitazione dell'ottimizzazione automatica\n",
    "        self.automatic_optimization = False\n",
    " \n",
    "    def forward(self, x, mode='A2B'):\n",
    "        if mode == 'A2B':\n",
    "            return self.netG_A2B(x)\n",
    "        else:\n",
    "            return self.netG_B2A(x)\n",
    " \n",
    "    def configure_optimizers(self):\n",
    "        # Ci servono 3 optimizer, ciascuno con il suo scheduler\n",
    "        optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A2B.parameters(), self.netG_B2A.parameters()),\n",
    "                                       lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2))\n",
    "        optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2))\n",
    "        optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2))\n",
    " \n",
    "        lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda epoch: 1.0 - max(0, epoch - self.hparams.decay_epoch) / (self.hparams.n_epochs - self.hparams.decay_epoch))\n",
    "        lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda epoch: 1.0 - max(0, epoch - self.hparams.decay_epoch) / (self.hparams.n_epochs - self.hparams.decay_epoch))\n",
    "        lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda epoch: 1.0 - max(0, epoch - self.hparams.decay_epoch) / (self.hparams.n_epochs - self.hparams.decay_epoch))\n",
    " \n",
    "        return [optimizer_G, optimizer_D_A, optimizer_D_B], [lr_scheduler_G, lr_scheduler_D_A, lr_scheduler_D_B]\n",
    " \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        real_A, real_B = batch\n",
    "        target_real = torch.ones((real_A.shape[0], 1)).type_as(real_A)\n",
    "        target_fake = torch.zeros((real_A.shape[0], 1)).type_as(real_A)\n",
    " \n",
    "        optimizers = self.optimizers()\n",
    "\n",
    "        optimizer_G = optimizers[0]\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Training generator\n",
    "        same_B = self.netG_B2A(real_B)\n",
    "        loss_identity = self.criterion_identity(same_B, real_B) * 5.0\n",
    "\n",
    "        same_A = self.netG_A2B(real_A)\n",
    "        loss_identity += self.criterion_identity(same_A, real_A) * 5.0\n",
    "\n",
    "        fake_B = self.netG_A2B(real_A)\n",
    "        pred_fake = self.netD_B(fake_B)\n",
    "        loss_GAN_A2B = self.criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        fake_A = self.netG_B2A(real_B)\n",
    "        pred_fake = self.netD_A(fake_A)\n",
    "        loss_GAN_B2A = self.criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        recovered_A = self.netG_B2A(fake_B)\n",
    "        loss_cycle_ABA = self.criterion_cycle(recovered_A, real_A) * 10.0\n",
    "\n",
    "        recovered_B = self.netG_A2B(fake_A)\n",
    "        loss_cycle_BAB = self.criterion_cycle(recovered_B, real_B) * 10.0\n",
    "\n",
    "        loss_G = loss_identity + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        self.manual_backward(loss_G)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        self.log(\"loss_G/overall\", loss_G)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            grid_A = torchvision.utils.make_grid(real_A[:50], nrow=8, normalize=True)\n",
    "            grid_A2B = torchvision.utils.make_grid(fake_B[:50], nrow=8, normalize=True)\n",
    "            grid_ABA = torchvision.utils.make_grid(recovered_A[:50], nrow=8, normalize=True)\n",
    "\n",
    "            grid_B = torchvision.utils.make_grid(real_B[:50], nrow=8, normalize=True)\n",
    "            grid_B2A = torchvision.utils.make_grid(fake_A[:50], nrow=8, normalize=True)\n",
    "            grid_BAB = torchvision.utils.make_grid(recovered_B[:50], nrow=8, normalize=True)\n",
    "\n",
    "            self.logger.experiment.add_image(\"A/A\", grid_A, self.global_step)\n",
    "            self.logger.experiment.add_image(\"A/A2B\", grid_A2B, self.global_step)\n",
    "            self.logger.experiment.add_image(\"A/ABA\", grid_ABA, self.global_step)\n",
    "\n",
    "            self.logger.experiment.add_image(\"B/B\", grid_B, self.global_step)\n",
    "            self.logger.experiment.add_image(\"B/B2A\", grid_B2A, self.global_step)\n",
    "            self.logger.experiment.add_image(\"B/BAB\", grid_BAB, self.global_step)\n",
    "\n",
    "        # Training discriminator A\n",
    "        optimizer_D_A = optimizers[1]\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        fake_real = self.netD_A(real_A)\n",
    "        loss_D_real = self.criterion_GAN(fake_real, target_real)\n",
    "\n",
    "        fake_A = self.netG_B2A(real_B).detach()\n",
    "        pred_fake = self.netD_A(fake_A)\n",
    "        loss_D_fake = self.criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        self.manual_backward(loss_D_A)\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        self.log(\"loss_D/loss_D_A\", loss_D_A)\n",
    "\n",
    "        # Training discriminator B\n",
    "        optimizer_D_B = optimizers[2]\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        loss_D_real = self.criterion_GAN(self.netD_B(real_B), target_real)\n",
    "\n",
    "        fake_B = self.netG_A2B(real_A).detach()\n",
    "        loss_D_fake = self.criterion_GAN(self.netD_B(fake_B), target_fake)\n",
    "\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        self.manual_backward(loss_D_B)\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        self.log(\"loss_D/loss_D_B\", loss_D_B)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((640,640),Image.BICUBIC),\n",
    "            transforms.RandomCrop(self.hparams.image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)) ## Ma qui non dovrei calcolare media e dev std dei dati?\n",
    "        ])\n",
    "\n",
    "        dataloader = DataLoader(ImageDataset(self.hparams.data_root, transform=transform), \n",
    "                                batch_size=self.hparams.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=4)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b67685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:27.040337Z",
     "iopub.status.busy": "2025-02-23T15:33:27.039976Z",
     "iopub.status.idle": "2025-02-23T15:33:30.334171Z",
     "shell.execute_reply": "2025-02-23T15:33:30.333464Z"
    },
    "papermill": {
     "duration": 3.2991,
     "end_time": "2025-02-23T15:33:30.335644",
     "exception": false,
     "start_time": "2025-02-23T15:33:27.036544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod=CycleGAN.load_from_checkpoint('./models/cycleGAN/epoch=4-step=21270.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8521b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T15:33:30.342724Z",
     "iopub.status.busy": "2025-02-23T15:33:30.342447Z",
     "iopub.status.idle": "2025-02-23T17:45:20.646269Z",
     "shell.execute_reply": "2025-02-23T17:45:20.645395Z"
    },
    "papermill": {
     "duration": 7910.309535,
     "end_time": "2025-02-23T17:45:20.648354",
     "exception": false,
     "start_time": "2025-02-23T15:33:30.338819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Classe per caricare immagini da più cartelle\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dirs, transform=None):\n",
    "        self.image_files = []  # Lista completa dei percorsi e nomi dei file\n",
    "        for image_dir in image_dirs:\n",
    "            files = sorted(os.listdir(image_dir))\n",
    "            self.image_files.extend([(os.path.join(image_dir, f), f) for f in files])  # Salva percorso e nome file\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, filename = self.image_files[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Carica immagine\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, filename  # Restituisce immagine e nome file originale\n",
    "\n",
    "# Definisci le cartelle delle immagini\n",
    "folder_A = \"./datasets/fullSynth/train/images\"  \n",
    "folder_B = \"./datasets/fullSynth/val/images\"  \n",
    "\n",
    "# Trasformazioni per il dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converti in tensore\n",
    "])\n",
    "\n",
    "# Carica il dataset unendo le immagini delle due cartelle\n",
    "dset = ImageDataset([folder_A, folder_B], transform)\n",
    "\n",
    "# Cartella di output\n",
    "output_dir = \"adaptedSynth\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Funzione per salvare le immagini trasformate\n",
    "def save_adapted_images():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mod.netG_A2B.to(device)\n",
    "\n",
    "    for i in range(len(dset)):\n",
    "        image_in, filename = dset[i]  # Ottieni immagine e nome file\n",
    "        image_in = image_in.to(device)\n",
    "\n",
    "        a2b = mod.netG_A2B(image_in.unsqueeze(0))\n",
    "\n",
    "        # Converti l'immagine in formato salvabile\n",
    "        a2b = a2b.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        a2b -= a2b.min()\n",
    "        a2b /= a2b.max()\n",
    "        a2b = (a2b * 255).astype(np.uint8)\n",
    "\n",
    "        # Salva l'immagine trasformata con lo stesso nome originale\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        Image.fromarray(a2b).save(output_path)\n",
    "\n",
    "        #print(f\"Salvata: {output_path}\")\n",
    "\n",
    "save_adapted_images()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6694541,
     "sourceId": 10806806,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 223766338,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7933.523869,
   "end_time": "2025-02-23T17:45:22.380566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-23T15:33:08.856697",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
